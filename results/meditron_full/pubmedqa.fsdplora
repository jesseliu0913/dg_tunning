nohup: ignoring input
2024-10-30:18:11:17,135 INFO     [__main__.py:279] Verbosity set to INFO
2024-10-30:18:11:23,103 INFO     [__init__.py:459] The tag 'arc_ca' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.
2024-10-30:18:11:23,110 INFO     [__init__.py:459] The tag 'arc_ca' is already registered as a group, this tag will not be registered. This may affect tasks you want to call.
2024-10-30:18:11:26,317 INFO     [__main__.py:376] Selected Tasks: ['pubmedqa']
2024-10-30:18:11:26,319 INFO     [evaluator.py:164] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2024-10-30:18:11:26,319 INFO     [evaluator.py:201] Initializing hf model, with arguments: {'pretrained': 'epfl-llm/meditron-7b', 'dtype': 'float16', 'low_cpu_mem_usage': True, 'trust_remote_code': True, 'peft': '/mnt/dg_tunning/lora4_meditron_7b'}
2024-10-30:18:11:26,563 WARNING  [other.py:349] Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2024-10-30:18:11:26,563 INFO     [huggingface.py:129] Using device 'cuda'
2024-10-30:18:11:26,645 INFO     [huggingface.py:481] Using model type 'default'
2024-10-30:18:11:26,773 INFO     [huggingface.py:365] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:09,  1.29s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:07,  1.27s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:03<00:05,  1.17s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:04<00:04,  1.12s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:05<00:03,  1.11s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:06<00:02,  1.09s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:07<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.02s/it]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 450 examples [00:00, 7332.73 examples/s]
Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 50 examples [00:00, 5970.54 examples/s]
Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 500 examples [00:00, 7824.81 examples/s]
2024-10-30:18:11:36,596 WARNING  [evaluator.py:270] Overwriting default num_fewshot of pubmedqa from None to 3
2024-10-30:18:11:36,596 WARNING  [model.py:422] model.chat_template was called with the chat_template set to False or None. Therefore no chat template will be applied. Make sure this is an intended behavior.
2024-10-30:18:11:36,597 INFO     [task.py:415] Building contexts for pubmedqa on rank 0...
  0%|          | 0/500 [00:00<?, ?it/s]100%|██████████| 500/500 [00:00<00:00, 31051.90it/s]
2024-10-30:18:11:36,661 INFO     [evaluator.py:489] Running loglikelihood requests
Running loglikelihood requests:   0%|          | 0/1500 [00:00<?, ?it/s]Running loglikelihood requests:   0%|          | 1/1500 [01:00<25:05:34, 60.26s/it]Running loglikelihood requests:   3%|▎         | 49/1500 [01:07<24:36,  1.02s/it]  Running loglikelihood requests:   6%|▋         | 97/1500 [01:14<11:52,  1.97it/s]Running loglikelihood requests:  10%|▉         | 145/1500 [01:21<07:43,  2.93it/s]Running loglikelihood requests:  13%|█▎        | 193/1500 [01:28<05:42,  3.82it/s]Running loglikelihood requests:  16%|█▌        | 241/1500 [01:34<04:32,  4.62it/s]Running loglikelihood requests:  19%|█▉        | 289/1500 [01:41<03:47,  5.33it/s]Running loglikelihood requests:  22%|██▏       | 337/1500 [01:47<03:16,  5.91it/s]Running loglikelihood requests:  26%|██▌       | 385/1500 [01:53<02:54,  6.39it/s]Running loglikelihood requests:  29%|██▉       | 433/1500 [01:59<02:37,  6.78it/s]Running loglikelihood requests:  32%|███▏      | 481/1500 [02:05<02:23,  7.09it/s]Running loglikelihood requests:  35%|███▌      | 529/1500 [02:11<02:12,  7.35it/s]Running loglikelihood requests:  38%|███▊      | 577/1500 [02:17<02:02,  7.55it/s]Running loglikelihood requests:  42%|████▏     | 625/1500 [02:23<01:53,  7.73it/s]Running loglikelihood requests:  45%|████▍     | 673/1500 [02:29<01:45,  7.87it/s]Running loglikelihood requests:  48%|████▊     | 721/1500 [02:35<01:37,  8.01it/s]Running loglikelihood requests:  51%|█████▏    | 769/1500 [02:40<01:30,  8.12it/s]Running loglikelihood requests:  54%|█████▍    | 817/1500 [02:46<01:23,  8.23it/s]Running loglikelihood requests:  58%|█████▊    | 865/1500 [02:52<01:16,  8.33it/s]Running loglikelihood requests:  61%|██████    | 913/1500 [02:57<01:09,  8.45it/s]Running loglikelihood requests:  64%|██████▍   | 961/1500 [03:03<01:02,  8.56it/s]Running loglikelihood requests:  67%|██████▋   | 1009/1500 [03:08<00:56,  8.65it/s]Running loglikelihood requests:  70%|███████   | 1057/1500 [03:13<00:50,  8.76it/s]Running loglikelihood requests:  74%|███████▎  | 1105/1500 [03:18<00:44,  8.89it/s]Running loglikelihood requests:  77%|███████▋  | 1153/1500 [03:24<00:38,  9.01it/s]Running loglikelihood requests:  80%|████████  | 1201/1500 [03:29<00:32,  9.15it/s]Running loglikelihood requests:  83%|████████▎ | 1249/1500 [03:34<00:27,  9.30it/s]Running loglikelihood requests:  86%|████████▋ | 1297/1500 [03:39<00:21,  9.45it/s]Running loglikelihood requests:  90%|████████▉ | 1345/1500 [03:43<00:16,  9.65it/s]Running loglikelihood requests:  93%|█████████▎| 1393/1500 [03:48<00:10,  9.89it/s]Running loglikelihood requests:  96%|█████████▌| 1441/1500 [03:52<00:05, 10.24it/s]Running loglikelihood requests:  99%|█████████▉| 1489/1500 [03:53<00:00, 13.41it/s]Running loglikelihood requests: 100%|██████████| 1500/1500 [03:53<00:00,  6.42it/s]
2024-10-30:18:15:36,901 WARNING  [huggingface.py:1353] Failed to get model SHA for /mnt/dg_tunning/lora4_meditron_7b at revision main. Error: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/dg_tunning/lora4_meditron_7b'. Use `repo_type` argument if needed.
2024-10-30:18:15:39,990 INFO     [evaluation_tracker.py:269] Output path not provided, skipping saving results aggregated
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 16
hf (pretrained=epfl-llm/meditron-7b,dtype=float16,low_cpu_mem_usage=True,trust_remote_code=True,peft=/mnt/dg_tunning/lora4_meditron_7b), gen_kwargs: (None), limit: None, num_fewshot: 3, batch_size: auto (16)
| Tasks  |Version|Filter|n-shot|Metric|   |Value|   |Stderr|
|--------|------:|------|-----:|------|---|----:|---|-----:|
|pubmedqa|      1|none  |     3|acc   |↑  |0.734|±  |0.0198|

