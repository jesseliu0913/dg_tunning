current model is: Qwen/Qwen2.5-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.05it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.36it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.31it/s]
/home/xinyuzh/anaconda3/envs/meditron/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/playpen/xinyu/Jesse/dg_tunning/exp1/tune_dialogue/tune_umls_dialogue.py:221: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: l-zijie. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /playpen/xinyu/Jesse/dg_tunning/exp1/tune_dialogue/wandb/run-20241231_002401-8toku9ao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ./qwen_dialogue_results
wandb: â­ï¸ View project at https://wandb.ai/l-zijie/huggingface
wandb: ðŸš€ View run at https://wandb.ai/l-zijie/huggingface/runs/8toku9ao
  0%|          | 0/15536 [00:00<?, ?it/s]  0%|          | 1/15536 [00:10<45:42:07, 10.59s/it]  0%|          | 2/15536 [00:20<42:43:05,  9.90s/it]  0%|          | 3/15536 [00:30<44:05:58, 10.22s/it]  0%|          | 4/15536 [00:41<44:36:15, 10.34s/it]  0%|          | 5/15536 [00:52<46:17:40, 10.73s/it]  0%|          | 6/15536 [01:02<45:40:07, 10.59s/it]  0%|          | 7/15536 [01:13<46:05:09, 10.68s/it]  0%|          | 8/15536 [01:23<45:17:38, 10.50s/it]  0%|          | 9/15536 [01:34<45:28:51, 10.54s/it]  0%|          | 10/15536 [01:44<45:10:09, 10.47s/it]                                                     {'loss': 1.4095, 'grad_norm': 1.0019474029541016, 'learning_rate': 1.287001287001287e-07, 'epoch': 0.0}
  0%|          | 10/15536 [01:44<45:10:09, 10.47s/it]  0%|          | 11/15536 [01:53<43:25:00, 10.07s/it]  0%|          | 12/15536 [02:03<42:39:59,  9.89s/it]  0%|          | 13/15536 [02:14<44:26:32, 10.31s/it]  0%|          | 14/15536 [02:23<43:01:08,  9.98s/it]  0%|          | 15/15536 [02:33<42:27:02,  9.85s/it]  0%|          | 16/15536 [02:43<42:42:12,  9.91s/it]  0%|          | 17/15536 [02:52<41:20:32,  9.59s/it]  0%|          | 18/15536 [03:00<39:31:24,  9.17s/it]  0%|          | 19/15536 [03:09<38:56:08,  9.03s/it]  0%|          | 20/15536 [03:19<40:54:39,  9.49s/it]                                                     {'loss': 1.5375, 'grad_norm': 1.2311046123504639, 'learning_rate': 2.574002574002574e-07, 'epoch': 0.0}
  0%|          | 20/15536 [03:19<40:54:39,  9.49s/it]  0%|          | 21/15536 [03:28<39:26:43,  9.15s/it]  0%|          | 22/15536 [03:38<41:29:38,  9.63s/it]  0%|          | 23/15536 [03:48<41:35:05,  9.65s/it]E1231 00:27:53.483000 1779878 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -9) local_rank: 0 (pid: 1779961) of binary: /home/xinyuzh/anaconda3/envs/meditron/bin/python
Traceback (most recent call last):
  File "/home/xinyuzh/anaconda3/envs/meditron/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/xinyuzh/anaconda3/envs/meditron/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/xinyuzh/anaconda3/envs/meditron/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/xinyuzh/anaconda3/envs/meditron/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/xinyuzh/anaconda3/envs/meditron/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/xinyuzh/anaconda3/envs/meditron/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
tune_umls_dialogue.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-12-31_00:27:53
  host      : unites4.cs.unc.edu
  rank      : 0 (local_rank: 0)
  exitcode  : -9 (pid: 1779961)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 1779961
========================================================
