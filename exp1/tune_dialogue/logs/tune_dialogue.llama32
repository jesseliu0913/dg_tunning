current model is: meta-llama/Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.27it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.98it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.83it/s]
22143 2413
/home/jesseliu/miniconda3/envs/maze/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/playpen/jesse/dg_tunning/exp1/tune_dialogue/tune_dialogue.py:176: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: l-zijie. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /playpen/jesse/dg_tunning/exp1/tune_dialogue/wandb/run-20250428_184049-gx7q9ecx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ./model_weights/llama3.2_inter
wandb: â­ï¸ View project at https://wandb.ai/l-zijie/huggingface
wandb: ðŸš€ View run at https://wandb.ai/l-zijie/huggingface/runs/gx7q9ecx
  0%|          | 0/11072 [00:00<?, ?it/s]  0%|          | 1/11072 [00:01<4:03:44,  1.32s/it]  0%|          | 2/11072 [00:02<3:21:50,  1.09s/it]  0%|          | 3/11072 [00:02<2:50:31,  1.08it/s]  0%|          | 4/11072 [00:03<2:24:42,  1.27it/s]  0%|          | 5/11072 [00:04<2:36:59,  1.17it/s]  0%|          | 6/11072 [00:05<2:36:56,  1.18it/s]  0%|          | 7/11072 [00:06<2:40:08,  1.15it/s]  0%|          | 8/11072 [00:07<2:35:15,  1.19it/s]  0%|          | 9/11072 [00:07<2:24:27,  1.28it/s]  0%|          | 10/11072 [00:08<2:33:28,  1.20it/s]  0%|          | 11/11072 [00:09<2:29:21,  1.23it/s]  0%|          | 12/11072 [00:10<2:32:58,  1.20it/s]