current model is: meta-llama/Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:02,  1.17it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:01<00:01,  1.26it/s]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:02<00:00,  1.25it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.68it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.49it/s]
22143 2413
/home/jesseliu/miniconda3/envs/maze/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/playpen/jesse/dg_tunning/exp1/tune_dialogue/tune_dialogue.py:176: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: l-zijie. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /playpen/jesse/dg_tunning/exp1/tune_dialogue/wandb/run-20250428_184034-o17jsien
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ./model_weights/llama3.1_inter
wandb: â­ï¸ View project at https://wandb.ai/l-zijie/huggingface
wandb: ðŸš€ View run at https://wandb.ai/l-zijie/huggingface/runs/o17jsien
  0%|          | 0/16608 [00:00<?, ?it/s]  0%|          | 1/16608 [00:02<10:30:00,  2.28s/it]  0%|          | 2/16608 [00:04<9:48:50,  2.13s/it]   0%|          | 3/16608 [00:05<8:39:06,  1.88s/it]  0%|          | 4/16608 [00:07<7:29:06,  1.62s/it]  0%|          | 5/16608 [00:09<8:17:48,  1.80s/it]  0%|          | 6/16608 [00:11<8:25:43,  1.83s/it]  0%|          | 7/16608 [00:13<8:39:52,  1.88s/it]  0%|          | 8/16608 [00:14<8:23:47,  1.82s/it]  0%|          | 9/16608 [00:16<7:46:59,  1.69s/it]  0%|          | 10/16608 [00:18<8:18:48,  1.80s/it]  0%|          | 11/16608 [00:19<8:02:37,  1.74s/it]  0%|          | 12/16608 [00:21<8:35:58,  1.87s/it]  0%|          | 13/16608 [00:23<8:00:40,  1.74s/it]  0%|          | 14/16608 [00:24<7:31:08,  1.63s/it]