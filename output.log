nohup: ignoring input
W1030 11:14:35.989362 140295730153280 torch/distributed/run.py:779] 
W1030 11:14:35.989362 140295730153280 torch/distributed/run.py:779] *****************************************
W1030 11:14:35.989362 140295730153280 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1030 11:14:35.989362 140295730153280 torch/distributed/run.py:779] *****************************************
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:13,  1.87s/it]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:13,  1.95s/it]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:13,  1.88s/it]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:13,  1.95s/it]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:13,  1.93s/it]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:13,  1.95s/it]Loading checkpoint shards:  12%|â–ˆâ–Ž        | 1/8 [00:01<00:13,  1.97s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:03<00:11,  1.84s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:03<00:11,  1.85s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:03<00:11,  1.92s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:03<00:11,  1.89s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:03<00:11,  1.92s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:03<00:11,  1.92s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:03<00:11,  1.92s/it]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:05<00:08,  1.78s/it]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:05<00:08,  1.79s/it]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:05<00:09,  1.85s/it]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:05<00:09,  1.82s/it]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:05<00:09,  1.84s/it]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:05<00:09,  1.85s/it]Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:05<00:09,  1.84s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:07<00:07,  1.78s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:07<00:07,  1.80s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:07<00:07,  1.85s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:07<00:07,  1.82s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:07<00:07,  1.84s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:07<00:07,  1.84s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:07<00:07,  1.84s/it]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:08<00:05,  1.77s/it]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:08<00:05,  1.78s/it]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:09<00:05,  1.83s/it]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:09<00:05,  1.80s/it]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:09<00:05,  1.82s/it]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:09<00:05,  1.82s/it]Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:09<00:05,  1.82s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:10<00:03,  1.71s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:10<00:03,  1.72s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:10<00:03,  1.77s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:10<00:03,  1.74s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:10<00:03,  1.76s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:10<00:03,  1.76s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:10<00:03,  1.75s/it]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:12<00:01,  1.69s/it]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:12<00:01,  1.70s/it]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:12<00:01,  1.74s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.57s/it]
Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:12<00:01,  1.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.26s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.57s/it]
Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:12<00:01,  1.71s/it]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:12<00:01,  1.71s/it]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:12<00:01,  1.70s/it]/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.62s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.59s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.25s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.59s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.60s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.60s/it]
/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/mnt/dg_tunning/tune_combined.py:129: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/mnt/dg_tunning/tune_combined.py:129: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/mnt/dg_tunning/tune_combined.py:129: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/mnt/dg_tunning/tune_combined.py:129: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/mnt/dg_tunning/tune_combined.py:129: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/mnt/dg_tunning/tune_combined.py:129: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
/mnt/dg_tunning/tune_combined.py:129: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
fa8b5d290355:55629:55629 [0] NCCL INFO Bootstrap : Using eth0:172.17.0.2<0>
fa8b5d290355:55629:55629 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
fa8b5d290355:55629:55629 [0] NCCL INFO cudaDriverVersion 12040
NCCL version 2.20.5+cuda12.4
fa8b5d290355:55634:55634 [5] NCCL INFO cudaDriverVersion 12040
fa8b5d290355:55633:55633 [4] NCCL INFO cudaDriverVersion 12040
fa8b5d290355:55633:55633 [4] NCCL INFO Bootstrap : Using eth0:172.17.0.2<0>
fa8b5d290355:55634:55634 [5] NCCL INFO Bootstrap : Using eth0:172.17.0.2<0>
fa8b5d290355:55633:55633 [4] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
fa8b5d290355:55634:55634 [5] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
fa8b5d290355:55629:55747 [0] NCCL INFO Failed to open libibverbs.so[.1]
fa8b5d290355:55629:55747 [0] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
fa8b5d290355:55629:55747 [0] NCCL INFO Using non-device net plugin version 0
fa8b5d290355:55629:55747 [0] NCCL INFO Using network Socket
fa8b5d290355:55634:55748 [5] NCCL INFO Failed to open libibverbs.so[.1]
fa8b5d290355:55634:55748 [5] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
fa8b5d290355:55634:55748 [5] NCCL INFO Using non-device net plugin version 0
fa8b5d290355:55634:55748 [5] NCCL INFO Using network Socket
fa8b5d290355:55633:55749 [4] NCCL INFO Failed to open libibverbs.so[.1]
fa8b5d290355:55633:55749 [4] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
fa8b5d290355:55633:55749 [4] NCCL INFO Using non-device net plugin version 0
fa8b5d290355:55633:55749 [4] NCCL INFO Using network Socket
fa8b5d290355:55630:55630 [1] NCCL INFO cudaDriverVersion 12040
fa8b5d290355:55630:55630 [1] NCCL INFO Bootstrap : Using eth0:172.17.0.2<0>
fa8b5d290355:55630:55630 [1] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
fa8b5d290355:55631:55631 [2] NCCL INFO cudaDriverVersion 12040
fa8b5d290355:55631:55631 [2] NCCL INFO Bootstrap : Using eth0:172.17.0.2<0>
fa8b5d290355:55631:55631 [2] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
fa8b5d290355:55630:55750 [1] NCCL INFO Failed to open libibverbs.so[.1]
fa8b5d290355:55630:55750 [1] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
fa8b5d290355:55630:55750 [1] NCCL INFO Using non-device net plugin version 0
fa8b5d290355:55630:55750 [1] NCCL INFO Using network Socket
fa8b5d290355:55631:55751 [2] NCCL INFO Failed to open libibverbs.so[.1]
fa8b5d290355:55631:55751 [2] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
fa8b5d290355:55631:55751 [2] NCCL INFO Using non-device net plugin version 0
fa8b5d290355:55631:55751 [2] NCCL INFO Using network Socket
fa8b5d290355:55632:55632 [3] NCCL INFO cudaDriverVersion 12040
fa8b5d290355:55632:55632 [3] NCCL INFO Bootstrap : Using eth0:172.17.0.2<0>
fa8b5d290355:55632:55632 [3] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
fa8b5d290355:55635:55635 [6] NCCL INFO cudaDriverVersion 12040
fa8b5d290355:55635:55635 [6] NCCL INFO Bootstrap : Using eth0:172.17.0.2<0>
fa8b5d290355:55635:55635 [6] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation
fa8b5d290355:55632:55752 [3] NCCL INFO Failed to open libibverbs.so[.1]
fa8b5d290355:55632:55752 [3] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
fa8b5d290355:55632:55752 [3] NCCL INFO Using non-device net plugin version 0
fa8b5d290355:55632:55752 [3] NCCL INFO Using network Socket
fa8b5d290355:55635:55753 [6] NCCL INFO Failed to open libibverbs.so[.1]
fa8b5d290355:55635:55753 [6] NCCL INFO NET/Socket : Using [0]eth0:172.17.0.2<0>
fa8b5d290355:55635:55753 [6] NCCL INFO Using non-device net plugin version 0
fa8b5d290355:55635:55753 [6] NCCL INFO Using network Socket
fa8b5d290355:55635:55753 [6] NCCL INFO comm 0x5571e4699f80 rank 6 nranks 7 cudaDev 6 nvmlDev 7 busId 87000 commId 0x4d4b7ea161ca370d - Init START
fa8b5d290355:55632:55752 [3] NCCL INFO comm 0x55e83011d140 rank 3 nranks 7 cudaDev 3 nvmlDev 4 busId 84000 commId 0x4d4b7ea161ca370d - Init START
fa8b5d290355:55629:55747 [0] NCCL INFO comm 0x556acd80aac0 rank 0 nranks 7 cudaDev 0 nvmlDev 1 busId 45000 commId 0x4d4b7ea161ca370d - Init START
fa8b5d290355:55630:55750 [1] NCCL INFO comm 0x55ebc8837b80 rank 1 nranks 7 cudaDev 1 nvmlDev 2 busId 46000 commId 0x4d4b7ea161ca370d - Init START
fa8b5d290355:55631:55751 [2] NCCL INFO comm 0x5621aa3e7f40 rank 2 nranks 7 cudaDev 2 nvmlDev 3 busId 47000 commId 0x4d4b7ea161ca370d - Init START
fa8b5d290355:55634:55748 [5] NCCL INFO comm 0x560cbe1cde80 rank 5 nranks 7 cudaDev 5 nvmlDev 6 busId 86000 commId 0x4d4b7ea161ca370d - Init START
fa8b5d290355:55633:55749 [4] NCCL INFO comm 0x5561af7dfd00 rank 4 nranks 7 cudaDev 4 nvmlDev 5 busId 85000 commId 0x4d4b7ea161ca370d - Init START
fa8b5d290355:55634:55748 [5] NCCL INFO Setting affinity for GPU 6 to ffffffff,fffffffe,00000000,00000000
fa8b5d290355:55632:55752 [3] NCCL INFO Setting affinity for GPU 4 to ffffffff,fffffffe,00000000,00000000
fa8b5d290355:55634:55748 [5] NCCL INFO NVLS multicast support is not available on dev 5
fa8b5d290355:55632:55752 [3] NCCL INFO NVLS multicast support is not available on dev 3
fa8b5d290355:55631:55751 [2] NCCL INFO NVLS multicast support is not available on dev 2
fa8b5d290355:55635:55753 [6] NCCL INFO Setting affinity for GPU 7 to ffffffff,fffffffe,00000000,00000000
fa8b5d290355:55635:55753 [6] NCCL INFO NVLS multicast support is not available on dev 6
fa8b5d290355:55630:55750 [1] NCCL INFO NVLS multicast support is not available on dev 1
fa8b5d290355:55633:55749 [4] NCCL INFO Setting affinity for GPU 5 to ffffffff,fffffffe,00000000,00000000
fa8b5d290355:55633:55749 [4] NCCL INFO NVLS multicast support is not available on dev 4
fa8b5d290355:55629:55747 [0] NCCL INFO NVLS multicast support is not available on dev 0
fa8b5d290355:55635:55753 [6] NCCL INFO comm 0x5571e4699f80 rank 6 nRanks 7 nNodes 1 localRanks 7 localRank 6 MNNVL 0
fa8b5d290355:55635:55753 [6] NCCL INFO Trees [0] -1/-1/-1->6->5 [1] -1/-1/-1->6->5
fa8b5d290355:55634:55748 [5] NCCL INFO comm 0x560cbe1cde80 rank 5 nRanks 7 nNodes 1 localRanks 7 localRank 5 MNNVL 0
fa8b5d290355:55635:55753 [6] NCCL INFO P2P Chunksize set to 524288
fa8b5d290355:55633:55749 [4] NCCL INFO comm 0x5561af7dfd00 rank 4 nRanks 7 nNodes 1 localRanks 7 localRank 4 MNNVL 0
fa8b5d290355:55629:55747 [0] NCCL INFO comm 0x556acd80aac0 rank 0 nRanks 7 nNodes 1 localRanks 7 localRank 0 MNNVL 0
fa8b5d290355:55634:55748 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
fa8b5d290355:55632:55752 [3] NCCL INFO comm 0x55e83011d140 rank 3 nRanks 7 nNodes 1 localRanks 7 localRank 3 MNNVL 0
fa8b5d290355:55633:55749 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
fa8b5d290355:55634:55748 [5] NCCL INFO P2P Chunksize set to 524288
fa8b5d290355:55629:55747 [0] NCCL INFO Channel 00/02 :    0   2   1   3   5   6   4
fa8b5d290355:55631:55751 [2] NCCL INFO comm 0x5621aa3e7f40 rank 2 nRanks 7 nNodes 1 localRanks 7 localRank 2 MNNVL 0
fa8b5d290355:55633:55749 [4] NCCL INFO P2P Chunksize set to 524288
fa8b5d290355:55632:55752 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
fa8b5d290355:55630:55750 [1] NCCL INFO comm 0x55ebc8837b80 rank 1 nRanks 7 nNodes 1 localRanks 7 localRank 1 MNNVL 0
fa8b5d290355:55629:55747 [0] NCCL INFO Channel 01/02 :    0   2   1   3   5   6   4
fa8b5d290355:55632:55752 [3] NCCL INFO P2P Chunksize set to 524288
fa8b5d290355:55631:55751 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
fa8b5d290355:55629:55747 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
fa8b5d290355:55631:55751 [2] NCCL INFO P2P Chunksize set to 524288
fa8b5d290355:55630:55750 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
fa8b5d290355:55629:55747 [0] NCCL INFO P2P Chunksize set to 524288
fa8b5d290355:55630:55750 [1] NCCL INFO P2P Chunksize set to 524288
fa8b5d290355:55633:55749 [4] NCCL INFO Channel 00/0 : 4[5] -> 0[1] via P2P/CUMEM
fa8b5d290355:55633:55749 [4] NCCL INFO Channel 01/0 : 4[5] -> 0[1] via P2P/CUMEM
fa8b5d290355:55634:55748 [5] NCCL INFO Channel 00/0 : 5[6] -> 6[7] via P2P/CUMEM
fa8b5d290355:55630:55750 [1] NCCL INFO Channel 00/0 : 1[2] -> 3[4] via P2P/CUMEM
fa8b5d290355:55629:55747 [0] NCCL INFO Channel 00/0 : 0[1] -> 2[3] via P2P/CUMEM
fa8b5d290355:55630:55750 [1] NCCL INFO Channel 01/0 : 1[2] -> 3[4] via P2P/CUMEM
fa8b5d290355:55629:55747 [0] NCCL INFO Channel 01/0 : 0[1] -> 2[3] via P2P/CUMEM
fa8b5d290355:55634:55748 [5] NCCL INFO Channel 01/0 : 5[6] -> 6[7] via P2P/CUMEM
fa8b5d290355:55631:55751 [2] NCCL INFO Channel 00/0 : 2[3] -> 1[2] via P2P/CUMEM
fa8b5d290355:55635:55753 [6] NCCL INFO Channel 00/0 : 6[7] -> 4[5] via P2P/CUMEM
fa8b5d290355:55631:55751 [2] NCCL INFO Channel 01/0 : 2[3] -> 1[2] via P2P/CUMEM
fa8b5d290355:55635:55753 [6] NCCL INFO Channel 01/0 : 6[7] -> 4[5] via P2P/CUMEM
fa8b5d290355:55632:55752 [3] NCCL INFO Channel 00/0 : 3[4] -> 5[6] via P2P/CUMEM
fa8b5d290355:55632:55752 [3] NCCL INFO Channel 01/0 : 3[4] -> 5[6] via P2P/CUMEM
fa8b5d290355:55629:55747 [0] NCCL INFO Connected all rings
fa8b5d290355:55629:55747 [0] NCCL INFO Channel 00/0 : 0[1] -> 1[2] via P2P/CUMEM
fa8b5d290355:55629:55747 [0] NCCL INFO Channel 01/0 : 0[1] -> 1[2] via P2P/CUMEM
fa8b5d290355:55631:55751 [2] NCCL INFO Connected all rings
fa8b5d290355:55630:55750 [1] NCCL INFO Connected all rings
fa8b5d290355:55631:55751 [2] NCCL INFO Channel 00/0 : 2[3] -> 3[4] via P2P/CUMEM
fa8b5d290355:55630:55750 [1] NCCL INFO Channel 00/0 : 1[2] -> 2[3] via P2P/CUMEM
fa8b5d290355:55631:55751 [2] NCCL INFO Channel 01/0 : 2[3] -> 3[4] via P2P/CUMEM
fa8b5d290355:55630:55750 [1] NCCL INFO Channel 01/0 : 1[2] -> 2[3] via P2P/CUMEM
fa8b5d290355:55630:55750 [1] NCCL INFO Channel 00/0 : 1[2] -> 0[1] via P2P/CUMEM
fa8b5d290355:55634:55748 [5] NCCL INFO Connected all rings
fa8b5d290355:55632:55752 [3] NCCL INFO Connected all rings
fa8b5d290355:55633:55749 [4] NCCL INFO Connected all rings
fa8b5d290355:55635:55753 [6] NCCL INFO Connected all rings
fa8b5d290355:55635:55753 [6] NCCL INFO Channel 00/0 : 6[7] -> 5[6] via P2P/CUMEM
fa8b5d290355:55630:55750 [1] NCCL INFO Channel 01/0 : 1[2] -> 0[1] via P2P/CUMEM
fa8b5d290355:55632:55752 [3] NCCL INFO Channel 00/0 : 3[4] -> 4[5] via P2P/CUMEM
fa8b5d290355:55633:55749 [4] NCCL INFO Channel 00/0 : 4[5] -> 5[6] via P2P/CUMEM
fa8b5d290355:55632:55752 [3] NCCL INFO Channel 01/0 : 3[4] -> 4[5] via P2P/CUMEM
fa8b5d290355:55633:55749 [4] NCCL INFO Channel 01/0 : 4[5] -> 5[6] via P2P/CUMEM
fa8b5d290355:55635:55753 [6] NCCL INFO Channel 01/0 : 6[7] -> 5[6] via P2P/CUMEM
fa8b5d290355:55632:55752 [3] NCCL INFO Channel 00/0 : 3[4] -> 2[3] via P2P/CUMEM
fa8b5d290355:55634:55748 [5] NCCL INFO Channel 00/0 : 5[6] -> 4[5] via P2P/CUMEM
fa8b5d290355:55633:55749 [4] NCCL INFO Channel 00/0 : 4[5] -> 3[4] via P2P/CUMEM
fa8b5d290355:55632:55752 [3] NCCL INFO Channel 01/0 : 3[4] -> 2[3] via P2P/CUMEM
fa8b5d290355:55634:55748 [5] NCCL INFO Channel 01/0 : 5[6] -> 4[5] via P2P/CUMEM
fa8b5d290355:55633:55749 [4] NCCL INFO Channel 01/0 : 4[5] -> 3[4] via P2P/CUMEM
fa8b5d290355:55629:55747 [0] NCCL INFO Connected all trees
fa8b5d290355:55630:55750 [1] NCCL INFO Connected all trees
fa8b5d290355:55630:55750 [1] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
fa8b5d290355:55630:55750 [1] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
fa8b5d290355:55629:55747 [0] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
fa8b5d290355:55629:55747 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer

fa8b5d290355:55630:55759 [1] misc/shmutils.cc:72 NCCL WARN Error: failed to extend /dev/shm/nccl-9xUa1U to 5767524 bytes

fa8b5d290355:55630:55759 [1] misc/shmutils.cc:113 NCCL WARN Error while creating shared memory segment /dev/shm/nccl-9xUa1U (size 5767520)
fa8b5d290355:55630:55759 [1] NCCL INFO proxy.cc:1252 -> 2
fa8b5d290355:55630:55759 [1] NCCL INFO proxy.cc:1315 -> 2
fa8b5d290355:55630:55750 [1] NCCL INFO proxy.cc:1064 -> 2
fa8b5d290355:55630:55750 [1] NCCL INFO init.cc:1328 -> 2
fa8b5d290355:55630:55750 [1] NCCL INFO init.cc:1501 -> 2
fa8b5d290355:55630:55750 [1] NCCL INFO group.cc:64 -> 2 [Async thread]

fa8b5d290355:55629:55757 [0] misc/shmutils.cc:72 NCCL WARN Error: failed to extend /dev/shm/nccl-jT1KxT to 5767524 bytes

fa8b5d290355:55629:55757 [0] misc/shmutils.cc:113 NCCL WARN Error while creating shared memory segment /dev/shm/nccl-jT1KxT (size 5767520)
fa8b5d290355:55629:55757 [0] NCCL INFO proxy.cc:1252 -> 2
fa8b5d290355:55629:55757 [0] NCCL INFO proxy.cc:1315 -> 2
fa8b5d290355:55630:55630 [1] NCCL INFO group.cc:418 -> 2
fa8b5d290355:55629:55747 [0] NCCL INFO proxy.cc:1064 -> 2
fa8b5d290355:55630:55630 [1] NCCL INFO init.cc:1876 -> 2
fa8b5d290355:55629:55747 [0] NCCL INFO init.cc:1328 -> 2
fa8b5d290355:55629:55747 [0] NCCL INFO init.cc:1501 -> 2
fa8b5d290355:55629:55747 [0] NCCL INFO group.cc:64 -> 2 [Async thread]
fa8b5d290355:55629:55629 [0] NCCL INFO group.cc:418 -> 2
fa8b5d290355:55629:55629 [0] NCCL INFO init.cc:1876 -> 2
fa8b5d290355:55630:55759 [1] NCCL INFO [Service thread] Connection closed by localRank 1
fa8b5d290355:55629:55757 [0] NCCL INFO [Service thread] Connection closed by localRank 0
fa8b5d290355:55631:55751 [2] NCCL INFO Connected all trees
fa8b5d290355:55635:55753 [6] NCCL INFO Connected all trees
fa8b5d290355:55631:55751 [2] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
fa8b5d290355:55632:55752 [3] NCCL INFO Connected all trees
fa8b5d290355:55634:55748 [5] NCCL INFO Connected all trees
fa8b5d290355:55633:55749 [4] NCCL INFO Connected all trees
fa8b5d290355:55631:55751 [2] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
fa8b5d290355:55635:55753 [6] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
fa8b5d290355:55632:55752 [3] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
fa8b5d290355:55634:55748 [5] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
fa8b5d290355:55633:55749 [4] NCCL INFO threadThresholds 8/8/64 | 56/8/64 | 512 | 512
fa8b5d290355:55635:55753 [6] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
fa8b5d290355:55632:55752 [3] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
fa8b5d290355:55634:55748 [5] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
fa8b5d290355:55633:55749 [4] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer

fa8b5d290355:55631:55762 [2] misc/shmutils.cc:72 NCCL WARN Error: failed to extend /dev/shm/nccl-b3EOKy to 5767524 bytes

fa8b5d290355:55632:55756 [3] misc/shmutils.cc:72 NCCL WARN Error: failed to extend /dev/shm/nccl-fiQ54D to 5767524 bytes

fa8b5d290355:55631:55762 [2] misc/shmutils.cc:113 NCCL WARN Error while creating shared memory segment /dev/shm/nccl-b3EOKy (size 5767520)

fa8b5d290355:55633:55761 [4] misc/shmutils.cc:72 NCCL WARN Error: failed to extend /dev/shm/nccl-U4oHOU to 5767524 bytes

fa8b5d290355:55635:55754 [6] misc/shmutils.cc:72 NCCL WARN Error: failed to extend /dev/shm/nccl-gAIclk to 5767524 bytes

fa8b5d290355:55632:55756 [3] misc/shmutils.cc:113 NCCL WARN Error while creating shared memory segment /dev/shm/nccl-fiQ54D (size 5767520)

fa8b5d290355:55634:55755 [5] misc/shmutils.cc:72 NCCL WARN Error: failed to extend /dev/shm/nccl-N3Fi3j to 5767524 bytes
fa8b5d290355:55631:55762 [2] NCCL INFO proxy.cc:1252 -> 2

fa8b5d290355:55633:55761 [4] misc/shmutils.cc:113 NCCL WARN Error while creating shared memory segment /dev/shm/nccl-U4oHOU (size 5767520)

fa8b5d290355:55635:55754 [6] misc/shmutils.cc:113 NCCL WARN Error while creating shared memory segment /dev/shm/nccl-gAIclk (size 5767520)
fa8b5d290355:55632:55756 [3] NCCL INFO proxy.cc:1252 -> 2

fa8b5d290355:55634:55755 [5] misc/shmutils.cc:113 NCCL WARN Error while creating shared memory segment /dev/shm/nccl-N3Fi3j (size 5767520)
fa8b5d290355:55631:55762 [2] NCCL INFO proxy.cc:1315 -> 2
fa8b5d290355:55633:55761 [4] NCCL INFO proxy.cc:1252 -> 2
fa8b5d290355:55632:55756 [3] NCCL INFO proxy.cc:1315 -> 2
fa8b5d290355:55635:55754 [6] NCCL INFO proxy.cc:1252 -> 2
fa8b5d290355:55634:55755 [5] NCCL INFO proxy.cc:1252 -> 2
fa8b5d290355:55633:55761 [4] NCCL INFO proxy.cc:1315 -> 2
fa8b5d290355:55635:55754 [6] NCCL INFO proxy.cc:1315 -> 2
fa8b5d290355:55634:55755 [5] NCCL INFO proxy.cc:1315 -> 2
fa8b5d290355:55631:55751 [2] NCCL INFO proxy.cc:1064 -> 2
fa8b5d290355:55632:55752 [3] NCCL INFO proxy.cc:1064 -> 2
fa8b5d290355:55631:55751 [2] NCCL INFO init.cc:1328 -> 2
fa8b5d290355:55632:55752 [3] NCCL INFO init.cc:1328 -> 2
fa8b5d290355:55633:55749 [4] NCCL INFO proxy.cc:1064 -> 2
fa8b5d290355:55631:55751 [2] NCCL INFO init.cc:1501 -> 2
fa8b5d290355:55633:55749 [4] NCCL INFO init.cc:1328 -> 2
fa8b5d290355:55632:55752 [3] NCCL INFO init.cc:1501 -> 2
fa8b5d290355:55631:55751 [2] NCCL INFO group.cc:64 -> 2 [Async thread]
fa8b5d290355:55634:55748 [5] NCCL INFO proxy.cc:1064 -> 2
fa8b5d290355:55635:55753 [6] NCCL INFO proxy.cc:1064 -> 2
fa8b5d290355:55633:55749 [4] NCCL INFO init.cc:1501 -> 2
fa8b5d290355:55632:55752 [3] NCCL INFO group.cc:64 -> 2 [Async thread]
fa8b5d290355:55634:55748 [5] NCCL INFO init.cc:1328 -> 2
fa8b5d290355:55635:55753 [6] NCCL INFO init.cc:1328 -> 2
fa8b5d290355:55633:55749 [4] NCCL INFO group.cc:64 -> 2 [Async thread]
fa8b5d290355:55634:55748 [5] NCCL INFO init.cc:1501 -> 2
fa8b5d290355:55634:55748 [5] NCCL INFO group.cc:64 -> 2 [Async thread]
fa8b5d290355:55635:55753 [6] NCCL INFO init.cc:1501 -> 2
fa8b5d290355:55635:55753 [6] NCCL INFO group.cc:64 -> 2 [Async thread]
fa8b5d290355:55631:55631 [2] NCCL INFO group.cc:418 -> 2
fa8b5d290355:55632:55632 [3] NCCL INFO group.cc:418 -> 2
fa8b5d290355:55631:55631 [2] NCCL INFO init.cc:1876 -> 2
fa8b5d290355:55633:55633 [4] NCCL INFO group.cc:418 -> 2
fa8b5d290355:55634:55634 [5] NCCL INFO group.cc:418 -> 2
fa8b5d290355:55632:55632 [3] NCCL INFO init.cc:1876 -> 2
fa8b5d290355:55633:55633 [4] NCCL INFO init.cc:1876 -> 2
fa8b5d290355:55634:55634 [5] NCCL INFO init.cc:1876 -> 2
fa8b5d290355:55635:55635 [6] NCCL INFO group.cc:418 -> 2
fa8b5d290355:55635:55635 [6] NCCL INFO init.cc:1876 -> 2
fa8b5d290355:55632:55756 [3] NCCL INFO [Service thread] Connection closed by localRank 3
fa8b5d290355:55633:55761 [4] NCCL INFO [Service thread] Connection closed by localRank 4
fa8b5d290355:55631:55762 [2] NCCL INFO [Service thread] Connection closed by localRank 2
fa8b5d290355:55634:55755 [5] NCCL INFO [Service thread] Connection closed by localRank 5
fa8b5d290355:55635:55754 [6] NCCL INFO [Service thread] Connection closed by localRank 6
fa8b5d290355:55629:55629 [0] NCCL INFO comm 0x556acd80aac0 rank 0 nranks 7 cudaDev 0 busId 45000 - Abort COMPLETE
fa8b5d290355:55630:55630 [1] NCCL INFO comm 0x55ebc8837b80 rank 1 nranks 7 cudaDev 1 busId 46000 - Abort COMPLETE
[rank1]: Traceback (most recent call last):
[rank1]:   File "/mnt/dg_tunning/tune_combined.py", line 139, in <module>
[rank1]:     trainer.train()
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2122, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2264, in _inner_training_loop
[rank1]:     self.model = self.accelerator.prepare(self.model)
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1350, in prepare
[rank1]:     result = tuple(
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1351, in <genexpr>
[rank1]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1226, in _prepare_one
[rank1]:     return self.prepare_model(obj, device_placement=device_placement)
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1511, in prepare_model
[rank1]:     model = FSDP(model, **kwargs)
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 483, in __init__
[rank1]:     _auto_wrap(
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 102, in _auto_wrap
[rank1]:     _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank1]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank1]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank1]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank1]:   [Previous line repeated 6 more times]
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 562, in _recursive_wrap
[rank1]:     return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 491, in _wrap
[rank1]:     return wrapper_cls(module, **kwargs)
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[rank1]:     _init_param_handle_from_module(
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 596, in _init_param_handle_from_module
[rank1]:     _sync_module_params_and_buffers(
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 1094, in _sync_module_params_and_buffers
[rank1]:     _sync_params_and_buffers(
[rank1]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/utils.py", line 326, in _sync_params_and_buffers
[rank1]:     dist._broadcast_coalesced(
[rank1]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:275, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank1]: ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank1]: Last error:
[rank1]: Error while creating shared memory segment /dev/shm/nccl-9xUa1U (size 5767520)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/dg_tunning/tune_combined.py", line 139, in <module>
[rank0]:     trainer.train()
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2122, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2264, in _inner_training_loop
[rank0]:     self.model = self.accelerator.prepare(self.model)
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1350, in prepare
[rank0]:     result = tuple(
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1351, in <genexpr>
[rank0]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1226, in _prepare_one
[rank0]:     return self.prepare_model(obj, device_placement=device_placement)
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1511, in prepare_model
[rank0]:     model = FSDP(model, **kwargs)
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 483, in __init__
[rank0]:     _auto_wrap(
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 102, in _auto_wrap
[rank0]:     _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank0]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank0]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank0]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank0]:   [Previous line repeated 6 more times]
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 562, in _recursive_wrap
[rank0]:     return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 491, in _wrap
[rank0]:     return wrapper_cls(module, **kwargs)
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[rank0]:     _init_param_handle_from_module(
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 596, in _init_param_handle_from_module
[rank0]:     _sync_module_params_and_buffers(
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 1094, in _sync_module_params_and_buffers
[rank0]:     _sync_params_and_buffers(
[rank0]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/utils.py", line 326, in _sync_params_and_buffers
[rank0]:     dist._broadcast_coalesced(
[rank0]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:275, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank0]: ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank0]: Last error:
[rank0]: Error while creating shared memory segment /dev/shm/nccl-jT1KxT (size 5767520)
fa8b5d290355:55635:55635 [6] NCCL INFO comm 0x5571e4699f80 rank 6 nranks 7 cudaDev 6 busId 87000 - Abort COMPLETE
fa8b5d290355:55631:55631 [2] NCCL INFO comm 0x5621aa3e7f40 rank 2 nranks 7 cudaDev 2 busId 47000 - Abort COMPLETE
[rank6]: Traceback (most recent call last):
[rank6]:   File "/mnt/dg_tunning/tune_combined.py", line 139, in <module>
[rank6]:     trainer.train()
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2122, in train
[rank6]:     return inner_training_loop(
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2264, in _inner_training_loop
[rank6]:     self.model = self.accelerator.prepare(self.model)
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1350, in prepare
[rank6]:     result = tuple(
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1351, in <genexpr>
[rank6]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1226, in _prepare_one
[rank6]:     return self.prepare_model(obj, device_placement=device_placement)
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1511, in prepare_model
[rank6]:     model = FSDP(model, **kwargs)
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 483, in __init__
[rank6]:     _auto_wrap(
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 102, in _auto_wrap
[rank6]:     _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank6]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank6]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank6]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank6]:   [Previous line repeated 6 more times]
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 562, in _recursive_wrap
[rank6]:     return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 491, in _wrap
[rank6]:     return wrapper_cls(module, **kwargs)
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[rank6]:     _init_param_handle_from_module(
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 596, in _init_param_handle_from_module
[rank6]:     _sync_module_params_and_buffers(
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 1094, in _sync_module_params_and_buffers
[rank6]:     _sync_params_and_buffers(
[rank6]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/utils.py", line 326, in _sync_params_and_buffers
[rank6]:     dist._broadcast_coalesced(
[rank6]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:275, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank6]: ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank6]: Last error:
[rank6]: Error while creating shared memory segment /dev/shm/nccl-gAIclk (size 5767520)
fa8b5d290355:55634:55634 [5] NCCL INFO comm 0x560cbe1cde80 rank 5 nranks 7 cudaDev 5 busId 86000 - Abort COMPLETE
[rank2]: Traceback (most recent call last):
[rank2]:   File "/mnt/dg_tunning/tune_combined.py", line 139, in <module>
[rank2]:     trainer.train()
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2122, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2264, in _inner_training_loop
[rank2]:     self.model = self.accelerator.prepare(self.model)
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1350, in prepare
[rank2]:     result = tuple(
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1351, in <genexpr>
[rank2]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1226, in _prepare_one
[rank2]:     return self.prepare_model(obj, device_placement=device_placement)
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1511, in prepare_model
[rank2]:     model = FSDP(model, **kwargs)
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 483, in __init__
[rank2]:     _auto_wrap(
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 102, in _auto_wrap
[rank2]:     _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank2]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank2]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank2]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank2]:   [Previous line repeated 6 more times]
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 562, in _recursive_wrap
[rank2]:     return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 491, in _wrap
[rank2]:     return wrapper_cls(module, **kwargs)
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[rank2]:     _init_param_handle_from_module(
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 596, in _init_param_handle_from_module
[rank2]:     _sync_module_params_and_buffers(
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 1094, in _sync_module_params_and_buffers
[rank2]:     _sync_params_and_buffers(
[rank2]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/utils.py", line 326, in _sync_params_and_buffers
[rank2]:     dist._broadcast_coalesced(
[rank2]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:275, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank2]: ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank2]: Last error:
[rank2]: Error while creating shared memory segment /dev/shm/nccl-b3EOKy (size 5767520)
[rank5]: Traceback (most recent call last):
[rank5]:   File "/mnt/dg_tunning/tune_combined.py", line 139, in <module>
[rank5]:     trainer.train()
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2122, in train
[rank5]:     return inner_training_loop(
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2264, in _inner_training_loop
[rank5]:     self.model = self.accelerator.prepare(self.model)
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1350, in prepare
[rank5]:     result = tuple(
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1351, in <genexpr>
[rank5]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1226, in _prepare_one
[rank5]:     return self.prepare_model(obj, device_placement=device_placement)
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1511, in prepare_model
[rank5]:     model = FSDP(model, **kwargs)
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 483, in __init__
[rank5]:     _auto_wrap(
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 102, in _auto_wrap
[rank5]:     _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank5]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank5]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank5]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank5]:   [Previous line repeated 6 more times]
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 562, in _recursive_wrap
[rank5]:     return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 491, in _wrap
[rank5]:     return wrapper_cls(module, **kwargs)
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[rank5]:     _init_param_handle_from_module(
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 596, in _init_param_handle_from_module
[rank5]:     _sync_module_params_and_buffers(
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 1094, in _sync_module_params_and_buffers
[rank5]:     _sync_params_and_buffers(
[rank5]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/utils.py", line 326, in _sync_params_and_buffers
[rank5]:     dist._broadcast_coalesced(
[rank5]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:275, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank5]: ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank5]: Last error:
[rank5]: Error while creating shared memory segment /dev/shm/nccl-N3Fi3j (size 5767520)
fa8b5d290355:55633:55633 [4] NCCL INFO comm 0x5561af7dfd00 rank 4 nranks 7 cudaDev 4 busId 85000 - Abort COMPLETE
fa8b5d290355:55632:55632 [3] NCCL INFO comm 0x55e83011d140 rank 3 nranks 7 cudaDev 3 busId 84000 - Abort COMPLETE
[rank4]: Traceback (most recent call last):
[rank4]:   File "/mnt/dg_tunning/tune_combined.py", line 139, in <module>
[rank4]:     trainer.train()
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2122, in train
[rank4]:     return inner_training_loop(
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2264, in _inner_training_loop
[rank4]:     self.model = self.accelerator.prepare(self.model)
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1350, in prepare
[rank4]:     result = tuple(
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1351, in <genexpr>
[rank4]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1226, in _prepare_one
[rank4]:     return self.prepare_model(obj, device_placement=device_placement)
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1511, in prepare_model
[rank4]:     model = FSDP(model, **kwargs)
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 483, in __init__
[rank4]:     _auto_wrap(
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 102, in _auto_wrap
[rank4]:     _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank4]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank4]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank4]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank4]:   [Previous line repeated 6 more times]
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 562, in _recursive_wrap
[rank4]:     return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 491, in _wrap
[rank4]:     return wrapper_cls(module, **kwargs)
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[rank4]:     _init_param_handle_from_module(
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 596, in _init_param_handle_from_module
[rank4]:     _sync_module_params_and_buffers(
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 1094, in _sync_module_params_and_buffers
[rank4]:     _sync_params_and_buffers(
[rank4]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/utils.py", line 326, in _sync_params_and_buffers
[rank4]:     dist._broadcast_coalesced(
[rank4]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:275, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank4]: ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank4]: Last error:
[rank4]: Error while creating shared memory segment /dev/shm/nccl-U4oHOU (size 5767520)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/mnt/dg_tunning/tune_combined.py", line 139, in <module>
[rank3]:     trainer.train()
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2122, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/transformers/trainer.py", line 2264, in _inner_training_loop
[rank3]:     self.model = self.accelerator.prepare(self.model)
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1350, in prepare
[rank3]:     result = tuple(
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1351, in <genexpr>
[rank3]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1226, in _prepare_one
[rank3]:     return self.prepare_model(obj, device_placement=device_placement)
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/accelerate/accelerator.py", line 1511, in prepare_model
[rank3]:     model = FSDP(model, **kwargs)
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 483, in __init__
[rank3]:     _auto_wrap(
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_wrap_utils.py", line 102, in _auto_wrap
[rank3]:     _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  # type: ignore[arg-type]
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank3]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank3]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 544, in _recursive_wrap
[rank3]:     wrapped_child, num_wrapped_params = _recursive_wrap(
[rank3]:   [Previous line repeated 6 more times]
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 562, in _recursive_wrap
[rank3]:     return _wrap(module, wrapper_cls, **kwargs), nonwrapped_numel
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/wrap.py", line 491, in _wrap
[rank3]:     return wrapper_cls(module, **kwargs)
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[rank3]:     _init_param_handle_from_module(
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 596, in _init_param_handle_from_module
[rank3]:     _sync_module_params_and_buffers(
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py", line 1094, in _sync_module_params_and_buffers
[rank3]:     _sync_params_and_buffers(
[rank3]:   File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/utils.py", line 326, in _sync_params_and_buffers
[rank3]:     dist._broadcast_coalesced(
[rank3]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:275, unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank3]: ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. 
[rank3]: Last error:
[rank3]: Error while creating shared memory segment /dev/shm/nccl-fiQ54D (size 5767520)
[rank0]:[W1030 11:15:00.302195219 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1030 11:15:04.035354 140295730153280 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 55629 closing signal SIGTERM
W1030 11:15:04.035922 140295730153280 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 55631 closing signal SIGTERM
W1030 11:15:04.036076 140295730153280 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 55632 closing signal SIGTERM
W1030 11:15:04.036228 140295730153280 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 55633 closing signal SIGTERM
W1030 11:15:04.037980 140295730153280 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 55634 closing signal SIGTERM
W1030 11:15:04.039620 140295730153280 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 55635 closing signal SIGTERM
E1030 11:15:04.569470 140295730153280 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 55630) of binary: /root/anaconda3/envs/meditron/bin/python
Traceback (most recent call last):
  File "/root/anaconda3/envs/meditron/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/anaconda3/envs/meditron/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
tune_combined.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-30_11:15:04
  host      : fa8b5d290355
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 55630)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
